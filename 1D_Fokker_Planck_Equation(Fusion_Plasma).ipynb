{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7fd055a4",
   "metadata": {},
   "source": [
    "<h4>References: </h4>\n",
    "\n",
    "[1]  R.J.Foldston et.al, J.Comp.Phys, 43(1981) 61\n",
    "[2]  https://farside.ph.utexas.edu/teaching/plasma/Plasma/node39.html\n",
    "<h4>1.According to Reference[1], Fokker-Planck operator for small angle binary collisions of test particles in a magnetized Maxwellian plasma is shown as: </h4>\n",
    "<h3>\n",
    "$\\left.\\frac{\\partial f}{\\partial t}\\right|_c=\\frac{1}{\\tau_s v^2} \\frac{\\partial}{\\partial v}\\left(v^3+v_c^3\\right) f+\\frac{1}{\\tau_s v^2} \\frac{\\partial}{\\partial v}\\left(\\frac{v^2 T_e}{m_b}+\\frac{v_c^3 T_i}{m_b v}\\right) \\frac{\\partial f}{\\partial v} + \\frac{v_{ii}}{2}\\frac{\\partial}{\\partial \\zeta}(1-\\zeta^2)\\frac{\\partial}{\\partial}f \\hspace{4mm}(1)$\n",
    "</h3>\n",
    "\n",
    "<h4 >2. Simplified formed when taking 1D-FP, ignoring the influence of the pitch angle : </h4>\n",
    "<h3>$\\left.\\frac{\\partial f}{\\partial t}\\right|_c=\\frac{1}{\\tau_s v^2} \\frac{\\partial}{\\partial v}\\left(v^3+v_c^3\\right) f+\\frac{1}{\\tau_s v^2} \\frac{\\partial}{\\partial v}\\left(\\frac{v^2 T_e}{m_b}+\\frac{v_c^3 T_i}{m_b v}\\right) \\frac{\\partial f}{\\partial v} \\hspace{4mm}(2)$\n",
    "</h3>\n",
    "\n",
    "Without CX loss and the Influence of an electric field, the equation can be expressed as:\n",
    "<h3>$\\frac{\\partial f}{\\partial t} = S_b + \\left.\\frac{\\partial f}{\\partial t}\\right|_c \\hspace{4mm}(3)$\n",
    "</h3>\n",
    "$S_b$ is the source term. The final form of equation to be discussed here can be expressed as:\n",
    "<h3>$\\frac{\\partial f}{\\partial t}=\\frac{1}{\\tau_s v^2} \\frac{\\partial}{\\partial v}\\left(v^3+v_c^3\\right) f+\\frac{1}{\\tau_s v^2}  \\frac{\\partial}{\\partial v}\\left[\\frac{T_e}{m_b v}\\left( v^3 +\\frac{T_i}{T_e}v_c^3\\right)\\frac{\\partial f}{\\partial v}\\right]+S_b \\hspace{4mm}(4)$\n",
    "</h3>\n",
    "The transformation of the second term on right side is performed with assuming an axisymmetric reactor and magnetic field($v \\cdot \\nabla f = 0$ and $v\\times B \\cdot \\frac{\\partial f}{\\partial v} = 0$)\n",
    "\n",
    "\n",
    "Denoting the beam energy at the time of injection as $E_{bo}[eV]$, according to $E_{bo} = \\frac{1}{2}m_bv_{bo}^2$, $v$ can be normalized to $v' = v/v_{bo}$, We have\n",
    "\n",
    "<h3>$\\frac{1}{\\tau_s v^2}\\frac{\\partial}{\\partial v}(v^3+v_c^3)f = \\frac{1}{\\tau_s v'^{2}}\\frac{\\partial}{\\partial v'}(v'^3+v_c'^3)f$</h3>\n",
    "\n",
    "<h3>$\\frac{1}{\\tau_s v^2}  \\frac{\\partial}{\\partial v}\\left[\\frac{T_e}{m_b v}\\left( v'^3 +\\frac{T_i}{T_e}v_c^3\\right)\\frac{\\partial f}{\\partial v}\\right] = \\frac{1}{\\tau_s v'^2}  \\frac{\\partial}{\\partial v'}\\left[\\frac{T_e}{m_b v_{bo}^2 v'}\\left( v'^3 +\\frac{T_i}{T_e}v_c'^3\\right)\\frac{\\partial f}{\\partial v'}\\right] = \\frac{1}{\\tau_s v'^2}  \\frac{\\partial}{\\partial v'}\\left[\\frac{T_e}{2E_{bo} v'}\\left( v'^3 +\\frac{T_i}{T_e}v_c'^3\\right)\\frac{\\partial f}{\\partial v'}\\right]$</h3>\n",
    "\n",
    "Thus, the function to discuss here can be normalized as:\n",
    "<h3>$\\frac{\\partial f}{\\partial t} = \\frac{1}{\\tau_s v'^{2}}\\frac{\\partial}{\\partial v'}(v'^3+v_c'^3)f + \\frac{1}{\\tau_s v'^2}  \\frac{\\partial}{\\partial v'}\\left[\\frac{T_e}{2E_{bo} v'}\\left( v'^3 +\\frac{T_i}{T_e}v_c'^3\\right)\\frac{\\partial f}{\\partial v'}\\right] + S_b$</h3>\n",
    "Where $v_c^{'3} = v_c^3/v_{bo}^3$\n",
    "\n",
    "<h4 >3. Defining constants : </h4>\n",
    "<h3>\n",
    "$\\tau_s = \\frac{6\\pi\\sqrt{2\\pi}\\epsilon_0^2m_em_b}{Z_b^2e^4n_eln{\\Lambda_e}}(\\frac{T_e}{m_e})^{\\frac{3}{2}} \\approx \\frac{6.32\\times10^8A_bT_e^{3/2}}{n_eZ_b^2ln\\Lambda_e}$,\n",
    "\n",
    "$v_{ii}=\\frac{\\langle Z \\rangle \\bar{A}_iv_c^3}{\\tau_s[Z]A_bv^3}$ (not needed in this case though) ,\n",
    "    \n",
    "$v_c^3 = 1.51 \\times 10^{20}T_e^{3/2}[Z]/\\bar{A}_i$,\n",
    "    \n",
    "$\\langle Z\\rangle = \\frac{\\Sigma_in_iZ_i^2ln\\Lambda_i}{n_eln\\Lambda_e}$    \n",
    "    \n",
    "$\\frac{[Z]}{\\bar{A}_i} = \\frac{\\Sigma_in_iZ_i^2ln\\Lambda_i/A_i}{n_eln\\Lambda_e}$  \n",
    "</h3>\n",
    "<h4>4.Basic constants define</h4>\n",
    "<h4>\n",
    "$n_e = 1 \\times 10^{20}   [\\frac{1}{m^3}]$\n",
    "    \n",
    "$T_e[\\mathrm{eV}] = 1.5 \\times 10^4 \\mathrm{eV}$\n",
    "\n",
    "$A_b = 2$ \n",
    "\n",
    "$Z_b = 1$ \n",
    "</h4>\n",
    "\n",
    "\n",
    "\n",
    "Coulomb logarithm $log\\Lambda_e$ is expressed as follows(Refs, [2])\n",
    "For thermal electron-electron collisions:\n",
    "$\\begin{array}{lr}\\ln \\Lambda_c=23-\\ln \\left(n_e^{1 / 2} T_e^{-3 / 2}\\right) & T_e<10 \\mathrm{eV}, \\\\ \\ln \\Lambda_c=24-\\ln \\left(n_e^{1 / 2} T_e^{-1}\\right) & T_e>10 \\mathrm{eV} .\\end{array}$\n",
    "\n",
    "For thermal electron-ion collisions in this case, we have:\n",
    "$\\begin{array}{ll}\\ln \\Lambda_c=30-\\ln \\left(n_e^{1 / 2} T_i^{-3 / 2} Z_i^{3 / 2} \\hat{m}_i\\right) & T_e<T_i m_e / m_i, \\\\ \\ln \\Lambda_c=23-\\ln \\left(n_e^{1 / 2} Z_i T_e^{-3 / 2}\\right) & T_i m_e / m_i<T_e<10 Z_i^2 \\mathrm{eV}, \\\\ \\ln \\Lambda_c=24-\\ln \\left(n_e^{1 / 2} T_e^{-1}\\right) & T_i m_e / m_i<10 Z_i^2 \\mathrm{eV}<T_e .\\end{array}$\n",
    "\n",
    "$n_e$ and $n_i$ are measured of $\\mathrm{cm}^{-3}$, $\\hat{m}_i = m_i/m_p$ where $m_p$ is the proton mass\n",
    "\n",
    "we assume that only   \n",
    "    \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b72f351",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_e = 3.0\n",
    "T_e = 15000\n",
    "loga_i = 1\n",
    "loga_e = 1\n",
    "za = (1)/(n_e * loga_e )\n",
    "vc = 1.51 * 1.0e20 * T_e **(3/2) * za\n",
    "\n",
    "A_b = 2.0  # NBi assumes to be Deuterium\n",
    "Z_b = 1.0\n",
    "tau_s = 6.32*1e8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1b3a944",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLP(n_input,  n_output, n_neuron, n_layer, act_fn):\n",
    "    input1 = keras.layers.Input(shape=(n_input,))\n",
    "    input2 = keras.layers.Input(shape=(n_input,))\n",
    "\n",
    "\n",
    "    merged = keras.layers.concatenate([input1, input2])\n",
    "    hidden_layers=[]\n",
    "\n",
    "    for _ in range(n_layer):\n",
    "        hidden = keras.layers.Dense(\n",
    "            units=n_neuron,\n",
    "            activation=act_fn,\n",
    "            kernel_initializer=tf.keras.initializers.GlorotNormal())\n",
    "        hidden_layers.append(hidden)\n",
    "\n",
    "    for hidden in hidden_layers:\n",
    "            merged = hidden(merged)\n",
    "\n",
    "\n",
    "    output = keras.layers.Dense(\n",
    "            units=n_output,\n",
    "            #activation=act_fn,\n",
    "            #kernel_initializer=tf.keras.initializers.GlorotNormal()\n",
    "            )(merged)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=[input1, input2], outputs=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0590218",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "\n",
    "    def __init__(self, patience=10, verbose=0):\n",
    "        self.stopped = 0\n",
    "        self.epoch = 0 \n",
    "        self.pre_loss = float('inf') \n",
    "        self.patience = patience\n",
    "        self.verbose = verbose # 早期終了メッセージの出力フラグをパラメーターで初期化\n",
    "\n",
    "    def __call__(self, current_loss):\n",
    "\n",
    "        if self.pre_loss <= current_loss: \n",
    "            self.epoch += 1 \n",
    "\n",
    "            if self.epoch > self.patience:\n",
    "                self.epoch = 0\n",
    "                self.pre_loss = current_loss \n",
    "                return True\n",
    "\n",
    "        else: \n",
    "            self.epoch = 0 \n",
    "            self.pre_loss = current_loss \n",
    "\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5039d48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhysicsInformedNNs():\n",
    "\n",
    "    def __init__(self, n_input, n_output, n_neuron, n_layer, epochs, loss_weights, act_fn='tanh'):\n",
    "        self.n_input = n_input\n",
    "        self.n_output = n_output\n",
    "        self.n_neuron = n_neuron\n",
    "        self.n_layer = n_layer\n",
    "        self.epochs = epochs\n",
    "        self.act_fn = act_fn\n",
    "        self.loss_weights = loss_weights\n",
    "        self.min_learning_rate = 1.0e-7\n",
    "\n",
    "    def build(self, optimizer, loss_fn, early_stopping, factor):\n",
    "        \n",
    "        self._model = MLP(self.n_input, self.n_output, self.n_neuron, self.n_layer, self.act_fn)\n",
    "        self._optimizer = optimizer\n",
    "        self._loss_fn = loss_fn\n",
    "        self._early_stopping = early_stopping\n",
    "        self._model_weights_save = [0, 0, self._model.get_weights()]\n",
    "        self._loss_min = float('inf')\n",
    "        self.factor = factor\n",
    "        return self\n",
    "\n",
    "    def train_step(self, x_boundary_data, x_boundary_data_y, p_data, x_pinn, y_pinn, z_pinn, coefficient, loop):\n",
    "\n",
    "        with tf.GradientTape() as tape_total:\n",
    "            tape_total.watch(self._model.trainable_variables)\n",
    "\n",
    "            # boundaries\n",
    "            p_pred = self._model([x_boundary_data,x_boundary_data_y, x_boundary_data_z])\n",
    "            loss11 = self._loss_fn(p_pred, p_data)\n",
    "            p_pred = self._model([y_boundary_data_x,y_boundary_data, y_boundary_data_z])\n",
    "            loss12 = self._loss_fn(p_pred, p_data)\n",
    "            p_pred = self._model([z_boundary_data_x,z_boundary_data_y, z_boundary_data])\n",
    "            loss13 = self._loss_fn(p_pred, p_data)\n",
    "\n",
    "            loss11 = tf.cast(loss11, dtype=tf.float32)\n",
    "            loss12 = tf.cast(loss12, dtype=tf.float32)\n",
    "            loss13 = tf.cast(loss13, dtype=tf.float32)\n",
    "\n",
    "            loss1 = loss11 + loss12 + loss13\n",
    "            loss1 = self.loss_weights[0]*tf.cast(loss1, dtype=tf.float32) # loss at boundaries\n",
    "            # main domain\n",
    "            with tf.GradientTape() as tapex:\n",
    "                tapex.watch(x_pinn)\n",
    "                with tf.GradientTape() as tapexx:\n",
    "                    tapexx.watch(x_pinn)\n",
    "                    p_pred_pinn = self._model([x_pinn, y_pinn, z_pinn]) # phi at x_pinn\n",
    "                dp_dx = tapexx.gradient(p_pred_pinn, x_pinn) # d(phi)/d(x_pinn)\n",
    "            dp_dx2 = tapex.gradient(dp_dx, x_pinn) # d^2(phi)/d(x_pinn)^2\n",
    "            dp_dx2 = tf.cast(dp_dx2, dtype=tf.float32)\n",
    "\n",
    "            with tf.GradientTape() as tapey:\n",
    "                tapey.watch(y_pinn)\n",
    "                with tf.GradientTape() as tapeyy:\n",
    "                    tapeyy.watch(y_pinn)\n",
    "                    p_pred_pinn = self._model([x_pinn, y_pinn, z_pinn])\n",
    "                dp_dy = tapeyy.gradient(p_pred_pinn, y_pinn)\n",
    "            dp_dy2 = tapey.gradient(dp_dy, y_pinn)\n",
    "            dp_dy2 = tf.cast(dp_dy2, dtype=tf.float32)\n",
    "\n",
    "            with tf.GradientTape() as tapez:\n",
    "                tapez.watch(z_pinn)\n",
    "                with tf.GradientTape() as tapezz:\n",
    "                    tapezz.watch(z_pinn)\n",
    "                    p_pred_pinn = self._model([x_pinn, y_pinn, z_pinn])\n",
    "                dp_dz = tapezz.gradient(p_pred_pinn, z_pinn)\n",
    "            dp_dz2 = tapez.gradient(dp_dz, z_pinn)\n",
    "            dp_dz2 = tf.cast(dp_dz2, dtype=tf.float32)\n",
    "\n",
    "            # cast variables as float32\n",
    "            p_pred_pinn = self._model([x_pinn, y_pinn, z_pinn])\n",
    "            p_pred_pinn = tf.cast(p_pred_pinn, dtype=tf.float32)\n",
    "\n",
    "            loss_physics = dp_dx2 + dp_dy2 + dp_dz2 + KB* p_pred_pinn # loss of the PDE\n",
    "            loss2 = self._loss_fn(loss_physics, tf.zeros_like(loss_physics)) # MSE of loss_physics; can be replaced with loss_physics^2\n",
    "            loss2 = self.loss_weights[1]*tf.cast(loss2, dtype=tf.float32)\n",
    "\n",
    "            # total loss\n",
    "            loss = loss1 + loss2\n",
    "\n",
    "        # Choose minimize\n",
    "        self._optimizer.minimize(loss, self._model.trainable_variables, tape=tape_total)\n",
    "        self._loss_values.append(loss)\n",
    "\n",
    "        # save weights and biases of the best model at this point\n",
    "        if loss < self._loss_min:\n",
    "            self._loss_min = loss\n",
    "            self._model_weights_save = [loop, loss, self._model.get_weights()]\n",
    "\n",
    "        if loop %50== 0:\n",
    "            print(\"train_step :\", self.loop,\"loss1 :\", loss1.numpy(), \"loss2 :\", loss2.numpy(), \"loss :\", loss.numpy())\n",
    "\n",
    "        return self\n",
    "\n",
    "    def train(self, x_boundary_data, x_boundary_data_y, p_data, x_pinn, y_pinn, z_pinn, coefficient):\n",
    "        \n",
    "        self.loop = 0\n",
    "        self._loss_values = []\n",
    "        self.stopped = 0\n",
    "        \n",
    "        for i in range(self.epochs):\n",
    "            self.loop += 1\n",
    "            self.train_step(x_boundary_data,\n",
    "                   x_boundary_data_y, x_boundary_data_z,\n",
    "                   y_boundary_data, y_boundary_data_x,\n",
    "                   y_boundary_data_z,z_boundary_data,\n",
    "                   z_boundary_data_x, z_boundary_data_y,\n",
    "                   p_data, x_pinn,y_pinn,z_pinn, KB, self.loop)\n",
    "            if self._early_stopping(self._loss_values[-1]):\n",
    "                self.stopped +=1\n",
    "                if self._optimizer.learning_rate * self.factor >= self.min_learning_rate:\n",
    "                    self._optimizer.learning_rate.assign(self._optimizer.learning_rate * self.factor)\n",
    "                    print(\"current_learning_rate:\" , self._optimizer.learning_rate)\n",
    "                if self.stopped >20:\n",
    "                    self._model.set_weights(self._model_weights_save[2]) # load weights and biases of the best model\n",
    "                    print('\\nLoss of the best model at {0}: {1} '.format(*self._model_weights_save))\n",
    "                    print(\"early stopping\")\n",
    "                    break\n",
    "                else:\n",
    "                    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d0de0c17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.497709829126027"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "n_e = 1.0e14\n",
    "T_e = 15000\n",
    "24-np.log(n_e**(1/2)*T_e**(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23565f8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
